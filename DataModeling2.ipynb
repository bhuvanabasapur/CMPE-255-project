{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset.\n",
    "con=sqlite3.connect('/Users/bhuvanagopalakrishnabasapur/PycharmProjects/Practise/Assignments/Wildfire_Project/Wildfire_project/FPA_FOD_20170508.sqlite')\n",
    "df = pd.read_sql_query(\"SELECT * FROM Fires\", con)\n",
    "pd.set_option('display.max_columns', None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_percentage(df):\n",
    "    \"\"\"This function takes a DataFrame(df) as input and returns two columns, total missing values and total missing values percentage\"\"\"\n",
    "    total = df.isnull().sum().sort_values(ascending = False)\n",
    "    percent = round(df.isnull().sum().sort_values(ascending = False)/len(df)*100,2)\n",
    "    return pd.concat([total, percent], axis=1, keys=['Total','Percent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_percentage(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping all those columns whose missing values are greater than 70%\n",
    "df = df.drop(['COMPLEX_NAME', 'MTBS_ID', 'MTBS_FIRE_NAME',\n",
    "            'ICS_209_INCIDENT_NUMBER', 'ICS_209_NAME', 'FIRE_CODE', 'LOCAL_FIRE_REPORT_ID'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the following columns as similar attributes are present with no missing values\n",
    "#CONT_TIME, FIRE_NAME, CONT_DOY, CONT_DATE, DISCOVERY_TIME\n",
    "df = df.drop(['CONT_TIME', 'CONT_DOY', 'CONT_DATE', 'DISCOVERY_TIME'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the columns which may not affect the model.\n",
    "df = df.drop(['FIRE_NAME', 'LOCAL_INCIDENT_ID', \n",
    "              'FIPS_NAME' , \n",
    "              'FIPS_CODE', 'NWCG_REPORTING_UNIT_NAME', \n",
    "              'NWCG_REPORTING_UNIT_ID','NWCG_REPORTING_AGENCY', \n",
    "              'SOURCE_REPORTING_UNIT', 'SOURCE_REPORTING_UNIT_NAME', \n",
    "              'SOURCE_SYSTEM','SOURCE_SYSTEM_TYPE', \n",
    "              'FPA_ID', 'FOD_ID', 'OWNER_CODE', 'OWNER_DESCR', \n",
    "              'COUNTY'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Observed that the vales in 'Shape' attribute are very long and not understandable. \n",
    "# As the shape does not affect the model will be removing Shape as well\n",
    "df = df.drop(['Shape'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the attribute STAT_CAUSE_DESCR as STAT_CAUSE_DESCR is the description of STAT_CAUSE_CODE\n",
    "df = df.drop(['STAT_CAUSE_DESCR'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the varibales X and y and converting them into numpy array\n",
    "\n",
    "#Changing pandas dataframe to numpy array\n",
    "y = df['FIRE_SIZE'].values\n",
    "X = np.concatenate( (df['DISCOVERY_DATE'].values.reshape(-1,1),df['DISCOVERY_DOY'].values.reshape(-1,1), \n",
    "                     df['STAT_CAUSE_CODE'].values.reshape(-1,1),df['LATITUDE'].values.reshape(-1,1), \n",
    "                     df['LONGITUDE'].values.reshape(-1,1)),axis = 1 )\n",
    "print(y[0:3])\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizing the data\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "sc = StandardScaler() \n",
    "X = sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary modules.\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import mean_absolute_error as MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#case 1\n",
    "#Training data size = 90%\n",
    "#Test data size = 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.10)\n",
    "print(\"# Train: {} , #Test: {}\".format(X_train.shape[0], X_test.shape[0]))\n",
    "print(\"# inputs: {}\".format(X_train.shape[1]))\n",
    "n = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y, bins=np.logspace(-4,6, 50));\n",
    "plt.ylabel('Number of Fires')\n",
    "plt.xlabel('Fire Size')\n",
    "plt.gca().set_yscale(\"log\")\n",
    "plt.gca().set_xscale(\"log\")\n",
    "plt.savefig(\"kaggle_fires_dist_log.png\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Mean fire size: {}\".format(np.mean(y)))\n",
    "print(\"Median fire size: {}\".format(np.median(y)))\n",
    "print(\"Var in fire size: {}\".format(np.var(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# This creates a LinearRegression object\n",
    "lm = LinearRegression()\n",
    "\n",
    "# Fit a linear model, calculate the root mean squared error \n",
    "# and the R2 score.\n",
    "lm.fit(X_train, y_train)\n",
    "\n",
    "y_train_predict  = lm.predict(X_train)\n",
    "y_test_predict = lm.predict(X_test)\n",
    "\n",
    "# Metrics for evaluation for train set.\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train, y_train_predict))\n",
    "r2_train = r2_score(y_train, y_train_predict)\n",
    "\n",
    "# Metrics for evaluation for test set.\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_test_predict))\n",
    "r2 = r2_score(y_test, y_test_predict)\n",
    "\n",
    "print('Root mean squared error on Training Set', rmse_train)\n",
    "print('R2 score on Training Set: ', r2_train)\n",
    "\n",
    "print('Root mean squared error on Testing Set', rmse)\n",
    "print('R2 score on Testing Set: ', r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    '''Writing a function to calculate Mean Absolute Percentage Error'''\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean absolute error (MAE).\n",
    "print('The MAE of the training set is ',\n",
    "       MAE(y_train, y_train_predict))\n",
    "print('The MAE of the testing set is ',\n",
    "       MAE(y_test, y_test_predict))\n",
    "# Mean absolute percentage error (MAPE).\n",
    "print('The MAPE of the training set is ', \n",
    "      mean_absolute_percentage_error(y_train, y_train_predict))\n",
    "print('The MAPE of the testing set is ',\n",
    "      mean_absolute_percentage_error(y_test, y_test_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted vs Actual scatterplot\n",
    "\n",
    "plt.scatter(y_test, y_test_predict, s=0.3, marker='.', c='r')\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Linear Regression')\n",
    "plt.gca().set_xscale(\"log\")\n",
    "plt.gca().set_yscale(\"log\")\n",
    "plt.plot([.1,20,1000],[.1,20,1000],c='k',linestyle='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from collections import OrderedDict\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Regressor\n",
    "clf = RandomForestRegressor(n_estimators=100,\n",
    "                               warm_start=True, oob_score=True,\n",
    "                               max_features=\"sqrt\", bootstrap = True,\n",
    "                               random_state=42)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting on the train set\n",
    "y_train_predicted_rf = clf.predict(X_train)\n",
    "# Predicting on the test set\n",
    "y_test_predicted_rf = clf.predict(X_test)\n",
    "\n",
    "# The Root mean squared error for train set.\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train, y_train_predicted_rf))\n",
    "# The Root mean squared error for test set.\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_test_predicted_rf))\n",
    "                    \n",
    "# R2 score for train set.\n",
    "r2_train_rf = r2_score(y_train, y_train_predicted_rf)\n",
    "# R2 score for test set.\n",
    "r2_test_rf = r2_score(y_test, y_test_predicted_rf)\n",
    "\n",
    "print(\"Root Mean squared error for train set: \", rmse_train)  \n",
    "print(\"Root Mean squared error for test set: \", rmse_test)  \n",
    "print('R2 score for train set: ', r2_train_rf)\n",
    "print('R2 score for test set: ', r2_test_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean absolute error (MAE).\n",
    "print('The MAE of the training set is ',\n",
    "       MAE(y_train, y_train_predicted_rf))\n",
    "print('The MAE of the testing set is ',\n",
    "       MAE(y_test, y_test_predicted_rf))\n",
    "\n",
    "# Mean absolute percentage error (MAPE) for Random Forest Regressor Model\n",
    "print('The MAPE of the training set is ', \n",
    "      mean_absolute_percentage_error(y_train, y_train_predicted_rf))\n",
    "print('The MAPE of the testing set is ',\n",
    "      mean_absolute_percentage_error(y_test, y_test_predicted_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted vs Actual scatterplot\n",
    "\n",
    "plt.scatter(y_test, y_test_predicted_rf, s=0.3, marker='.', c='r')\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Random Forest')\n",
    "plt.gca().set_xscale(\"log\")\n",
    "plt.gca().set_yscale(\"log\")\n",
    "plt.plot([.1,20,1000],[.1,20,1000],c='k',linestyle='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "from sklearn import svm\n",
    "\n",
    "n = 10000\n",
    "svm_reg = svm.SVR(kernel='rbf', degree=3, gamma='auto')\n",
    "\n",
    "svm_reg.fit(X_train[0:n,:],y_train[0:n].reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_predict_svm  = svm_reg.predict(X_train[0:n,:])\n",
    "y_test_predict_svm = svm_reg.predict(X_test[0:n,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics for evaluation for train set.\n",
    "rmse_train_svm = np.sqrt(mean_squared_error(y_train[0:n], y_train_predict_svm))\n",
    "r2_train_svm = r2_score(y_train[0:n], y_train_predict_svm)\n",
    "\n",
    "# Metrics for evaluation for test set.\n",
    "rmse_svm = np.sqrt(mean_squared_error(y_test[0:n], y_test_predict_svm))\n",
    "r2_svm = r2_score(y_test[0:n], y_test_predict_svm)\n",
    "\n",
    "print('Root mean squared error on Training Set', rmse_train_svm)\n",
    "print('R2 score on Training Set: ', r2_train_svm)\n",
    "\n",
    "print('Root mean squared error on Testing Set', rmse_svm)\n",
    "print('R2 score on Testing Set: ', r2_svm)\n",
    "print('----------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean absolute error (MAE).\n",
    "print('The MAE of the training set is ',\n",
    "       MAE(y_train[0:n], y_train_predict_svm))\n",
    "print('The MAE of the testing set is ',\n",
    "       MAE(y_test[0:n], y_test_predict_svm))\n",
    "\n",
    "# Mean absolute percentage error (MAPE).\n",
    "print('The MAPE of the training set is ', \n",
    "      mean_absolute_percentage_error(y_train[0:n], y_train_predict_svm))\n",
    "print('The MAPE of the testing set is ',\n",
    "      mean_absolute_percentage_error(y_test[0:n], y_test_predict_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted vs Actual scatterplot\n",
    "\n",
    "plt.scatter(y_test[0:n], y_test_predict_svm, s=0.3, marker='.', c='r')\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('SVM')\n",
    "plt.gca().set_xscale(\"log\")\n",
    "plt.gca().set_yscale(\"log\")\n",
    "plt.plot([.1,20,1000],[.1,20,1000],c='k',linestyle='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Knn\n",
    "# K Nearest Neighbours\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "n = 50000\n",
    "\n",
    "for nbs in [3,5,10,15,20]:\n",
    "    for wts in ['distance','uniform']:\n",
    "        K_nn = KNeighborsRegressor(n_neighbors=nbs,\n",
    "                                   weights=wts)\n",
    "\n",
    "        K_nn.fit(X_train[0:n,:],y_train[0:n].reshape(-1))\n",
    "        y_train_predict_knn = K_nn.predict(X_train[0:n,:])\n",
    "        y_test_predict_knn = K_nn.predict(X_test[0:n,:])\n",
    "\n",
    "        print(\"nbs: {}, wts: {}\".format(nbs, wts))\n",
    "        print(\"MAE Train: {}\".format(MAE(y_train[0:n],y_train_predict_knn)))\n",
    "        print(\"MAE Test: {}\".format(MAE(y_test[0:n],y_test_predict_knn)))\n",
    "        print(\"r2 Train: {}\".format(r2_score(y_train[0:n],y_train_predict_knn)))\n",
    "        print(\"r2 Test: {}\".format(r2_score(y_test[0:n],y_test_predict_knn)))\n",
    "        print(\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted vs Actual scatterplot\n",
    "\n",
    "plt.scatter(y_test[0:n], y_test_predict_knn, s=0.3, marker='.', c='r')\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Knn')\n",
    "plt.gca().set_xscale(\"log\")\n",
    "plt.gca().set_yscale(\"log\")\n",
    "plt.plot([.1,20,1000],[.1,20,1000],c='k',linestyle='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "from sklearn import tree\n",
    "\n",
    "tree_clf = tree.DecisionTreeRegressor(criterion='mae')\n",
    "\n",
    "tree_clf.fit(X_train[0:n,:],y_train[0:n].reshape(-1))\n",
    "\n",
    "y_train_predict_dt = tree_clf.predict(X_train[0:n,:])\n",
    "y_test_predict_dt = tree_clf.predict(X_test[0:n,:])\n",
    "\n",
    "print(\"MAE Train: {}\".format(MAE(y_train[0:n],y_train_predict_dt)))\n",
    "print(\"MAE Test: {}\".format(MAE(y_test[0:n],y_test_predict_dt)))\n",
    "print(\"r2 Train: {}\".format(r2_score(y_train[0:n],y_train_predict_dt)))\n",
    "print(\"r2 Test: {}\".format(r2_score(y_test[0:n],y_test_predict_dt)))\n",
    "print(\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted vs Actual scatterplot\n",
    "\n",
    "plt.scatter(y_test[0:n], y_test_predict_dt, s=0.3, marker='.', c='r')\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Decision Tree')\n",
    "plt.gca().set_xscale(\"log\")\n",
    "plt.gca().set_yscale(\"log\")\n",
    "plt.plot([.1,20,1000],[.1,20,1000],c='k',linestyle='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacked regressors\n",
    "from mlxtend.regressor import StackingRegressor\n",
    "\n",
    "n = 100000\n",
    "\n",
    "estimators = [lm, tree_clf, K_nn, svm_reg]\n",
    "\n",
    "regStack = StackingRegressor(regressors=estimators,\n",
    "                            meta_regressor = lm)\n",
    "\n",
    "regStack.fit(X_train[0:n,:],y_train[0:n].reshape(-1))\n",
    "\n",
    "y_train_predict_regStack = regStack.predict(X_train[0:n,:])\n",
    "y_test_predict_regStack = regStack.predict(X_test[0:n,:])\n",
    "\n",
    "print(\"MAE Train: {}\".format(MAE(y_train[0:n],y_train_predict_regStack)))\n",
    "print(\"MAE Test: {}\".format(MAE(y_test[0:n],y_test_predict_regStack)))\n",
    "print(\"r2 Train: {}\".format(r2_score(y_train[0:n],y_train_predict_regStack)))\n",
    "print(\"r2 Test: {}\".format(r2_score(y_test[0:n],y_test_predict_regStack)))\n",
    "print(\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted vs Actual scatterplot\n",
    "\n",
    "plt.scatter(y_test[0:n], y_test_predict_regStack, s=0.3, marker='.', c='r')\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Stacked Regressor')\n",
    "plt.gca().set_xscale(\"log\")\n",
    "plt.gca().set_yscale(\"log\")\n",
    "plt.plot([.1,20,1000],[.1,20,1000],c='k',linestyle='--')"
   ]
  }
 ]
}